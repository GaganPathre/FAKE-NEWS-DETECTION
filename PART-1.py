# -*- coding: utf-8 -*-
"""THESIS_MODEL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kqInWVbmEO9WU8CcJqUPMytffBflK3Km
"""

from google.colab import drive
drive.mount('/content/drive')

'''Implementation of Classic Machine Learnign Models'''

!pip install langdetect
import os
import re
from bs4 import BeautifulSoup
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from langdetect import detect
from sklearn.metrics import precision_recall_curve, roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from bs4 import MarkupResemblesLocatorWarning
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

path = kagglehub.dataset_download("doanquanvietnamca/liar-dataset")

print("Path to dataset files:", path)

!mv /root/.cache/kagglehub/datasets/doanquanvietnamca/liar-dataset/versions/1 /content

import kagglehub


path = kagglehub.dataset_download("mohamedgreshamahdi/fakenewsnet")

print("Path to dataset files:", path)

import kagglehub


path = kagglehub.dataset_download("csmalarkodi/isot-fake-news-dataset")

print("Path to dataset files:", path)

warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)

def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = BeautifulSoup(text, "html.parser").get_text()
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)
    text = re.sub(r"[^A-Za-z0-9\s.,!?']", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def is_english(text):
    try:
        return detect(text) == 'en'
    except:
        return False

def load_liar(path):
    parts = ["train.tsv", "test.tsv", "valid.tsv"]
    dfs = []
    for part in parts:
        df = pd.read_csv(os.path.join(path, part), sep='\t', header=None)
        df = df[[2, 1]]  # statement, label
        df.columns = ['text', 'label']
        dfs.append(df)
    df = pd.concat(dfs)
    label_map = {
        'true': 1, 'mostly-true': 1, 'half-true': 1,
        'barely-true': 0, 'false': 0, 'pants-fire': 0
    }
    df['label'] = df['label'].map(label_map)
    df.dropna(inplace=True)
    df['text'] = df['text'].apply(clean_text)
    return df[df['text'].apply(is_english)]

def load_isot(path):
    true_df = pd.read_csv(os.path.join(path, "True.csv"))
    fake_df = pd.read_csv(os.path.join(path, "Fake.csv"))
    true_df['label'] = 1
    fake_df['label'] = 0
    df = pd.concat([true_df[['text', 'label']], fake_df[['text', 'label']]])
    df['text'] = df['text'].apply(clean_text)
    return df[df['text'].apply(is_english)]

def load_fakenewsnet(path):
    file_map = {
        "BuzzFeed_real_news_content.csv": 1,
        "BuzzFeed_fake_news_content.csv": 0,
        "Politifact_real_news_content.csv": 1,
        "Politifact_fake_news_content.csv": 0,
        "gossipcop_real.csv": 1,
        "gossipcop_fake.csv": 0,
        "politifact_real.csv": 1,
        "politifact_fake.csv": 0
    }
    all_rows = []
    for fname, label in file_map.items():
        full_path = os.path.join(path, fname)
        if os.path.isfile(full_path):
            df = pd.read_csv(full_path)
            if 'text' in df.columns:
                df = df[['text']].copy()
            elif 'content' in df.columns:
                df.rename(columns={'content': 'text'}, inplace=True)
            else:
                continue
            df.dropna(inplace=True)
            df['text'] = df['text'].apply(clean_text)
            df = df[df['text'].apply(is_english)]
            df['label'] = label
            all_rows.append(df)
    return pd.concat(all_rows, ignore_index=True)

def main():
    liar_df = load_liar("/content/LIAR")
    isot_df = load_isot("/content/ISOT")
    fakenewsnet_df = load_fakenewsnet("/content/FakeNewsNet")

    print(f"LIAR: {len(liar_df)} | ISOT: {len(isot_df)} | FakeNewsNet: {len(fakenewsnet_df)}")

    merged_df = pd.concat([liar_df, isot_df, fakenewsnet_df], ignore_index=True)
    merged_df.drop_duplicates(subset='text', inplace=True)
    merged_df.to_csv("/content/DATASET.csv", index=False)

    print(f"Merged Dataset Size: {len(merged_df)} samples saved to /content/DATASET.csv")

if __name__ == "__main__":
    main()

df = pd.read_csv("/content/DATASET.csv")


print(df.info())
print(df['label'].value_counts())

# Bar plot of label distribution
plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='label')
plt.xticks([0, 1], ['Fake', 'Real'])
plt.title('Fake vs Real News Distribution')
plt.show()


df['length'] = df['text'].apply(lambda x: len(x.split()))
sns.histplot(data=df, x='length', hue='label', bins=50, kde=True)
plt.title('Text Length Distribution by Label')
plt.show()

# WordClouds
fake_text = ' '.join(df[df['label']==0]['text'].tolist())
real_text = ' '.join(df[df['label']==1]['text'].tolist())

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(WordCloud(max_words=100, background_color='white').generate(fake_text))
plt.axis('off')
plt.title("Fake News WordCloud")

plt.subplot(1, 2, 2)
plt.imshow(WordCloud(max_words=100, background_color='white').generate(real_text))
plt.axis('off')
plt.title("Real News WordCloud")

plt.tight_layout()
plt.show()



# Load data
df = pd.read_csv("/content/DATASET.csv")

X = df['text']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=5, ngram_range=(1, 2))
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)


# Feature extractor
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=3, ngram_range=(1,3))

X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Train Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train_vec, y_train)
lr_pred = lr.predict(X_test_vec)

# Train Random Forest
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train_vec, y_train)
rf_pred = rf.predict(X_test_vec)

# Predict on new text
new_texts = ["Elon Musk launches mind-control device for mass adoption"]
X = vectorizer.transform(new_texts)
pred = model.predict(X)
X_new = vectorizer.transform(new_texts)
predictions = model.predict(X_new)
print("Prediction:", predictions[0])
probs = model.predict_proba(X_new)
print(f"Fake: {probs[0][0]:.4f}, Real: {probs[0][1]:.4f}")


# Get prediction probabilities
lr_probs = lr.predict_proba(X_test_vec)[:,1]

precision, recall, _ = precision_recall_curve(y_test, lr_probs)
plt.plot(recall, precision, marker='.', label='Logistic Regression')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

fpr, tpr, _ = roc_curve(y_test, lr_probs)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0,1],[0,1],'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

